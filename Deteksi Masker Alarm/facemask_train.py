# -*- coding: utf-8 -*-
"""Facemask_Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fmWIFDJsMQJ9NRXyL-ilJV9ymAQIkIKn
"""
#Bisa ganti kode hapus tanda hastag.
#coba jalankan dikomputer saja mas
#install library yang tertera di script
#kecuali os itu sudah standard library
#Install library tensorflow
#keras
#sklearn
#imutils
#matplotlib
#numpy
#itertools
#hati-hati untuk ganti kode, kalau perlu chat whatsapp saya lagi

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model
#from keras.models import Sequential
#from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout
from tensorflow.keras.optimizers import Adam
from keras.callbacks import TensorBoard, ModelCheckpoint
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from sklearn.metrics import classification_report
from imutils import paths
import matplotlib.pyplot as plt
import numpy as np
import os
import itertools

"""#Data prepocessing"""

#model = Sequential([
    #Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),
    #MaxPooling2D(2,2),
    
    #Conv2D(100, (3,3), activation='relu'),
    #MaxPooling2D(2,2),
    
    #Flatten(),
   # Dropout(0.5),
  #  Dense(50, activation='relu'),
 #   Dense(2, activation='softmax')
#])
#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

trainpath = "/content/train_image/"
validpath = "/content/valid_train_image/"

#TRAINING_DIR = "/content/train_image/"
#train_datagen = ImageDataGenerator(rescale=1.0/255,
       #                            rotation_range=40,
      #                             width_shift_range=0.2,
     #                              height_shift_range=0.2,
    #                               shear_range=0.2,
   #                                zoom_range=0.2,
  #                                 horizontal_flip=True,
 #                                  fill_mode='nearest')

#train_generator = train_datagen.flow_from_directory(TRAINING_DIR, 
                                                    #batch_size=10, 
                                                    #target_size=(150, 150))
#VALIDATION_DIR = "/content/valid_train_image/"
#validation_datagen = ImageDataGenerator(rescale=1.0/255)

#validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, 
 #                                                        batch_size=10, 
  #                                                       target_size=(150, 150))

trainbatches = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    shear_range=0.15,
    rotation_range=20,
    zoom_range=2,
    horizontal_flip=True,
    width_shift_range=0.8,
    fill_mode="nearest"
).flow_from_directory(
    directory=trainpath,
    target_size=(224,224),
    batch_size=24,
    shuffle=True,
)

validbatches = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    shear_range=0.15,
    rotation_range=20,
    zoom_range=2,
    horizontal_flip=True,
    width_shift_range=0.8,
    fill_mode="nearest"
).flow_from_directory(
    directory=validpath,
    target_size=(224,224),
    batch_size=10,
)

#checkpoint = ModelCheckpoint('model2-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')

basemodel = MobileNetV2(weights="imagenet",
                    include_top=False,
                   input_tensor=Input(shape=(224,224,3)))

for layer in basemodel.layers:
  layer.trainable = False

# construct the head of the model that will be placed on top of the
# the base model
headModel = basemodel.output
headModel = AveragePooling2D(pool_size=(7,7))(headModel)
headModel = Flatten(name="flatten")(headModel)
headModel = Dense(128, activation="relu")(headModel)
headModel = Dropout(0.5)(headModel)
headModel = Dense(2, activation="softmax")(headModel)

# place the head FC model on top of the base model (this will become
# the actual model we will train)
model = Model(inputs=basemodel.input, outputs=headModel)

model.summary()

import tensorflow
metrics = [
    tensorflow.keras.metrics.Recall(),
    tensorflow.keras.metrics.Precision(),
    tensorflow.keras.metrics.BinaryAccuracy()
]
model.compile(optimizer=Adam(lr = 0.00001),loss="binary_crossentropy",metrics = ['acc'])
#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

"""#setting for binary classification (with_mask/without_mask)"""

#BUG dari Keras saya up ya
# Link url bug Keras official: https://github.com/keras-team/keras/issues/16406
#history = model.fit(
 #   train_generator,
  #  epochs=10,
  #  validation_data=validation_generator,
   # callbacks=[checkpoint]
   # )

#BUG dari Keras saya up ya
# Link url bug Keras official: https://github.com/keras-team/keras/issues/16406
#Pake yang mana saja tetap saja sama hasilnya bug official.
history = model.fit(
    trainbatches,
    #epochs=10,
    validation_data=validbatches,
    #verbose=1,
    steps_per_epoch= len(trainbatches), #3594//24,
    validation_steps= len(validbatches),#len(str(177//10))
    epochs=10,
    verbose=1
)

from matplotlib import pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val','train_acc','val_acc'], loc='upper left')
plt.show()

model.save("mask_detector.model", save_format="h5")

testbatch = ImageDataGenerator(
    preprocessing_function=preprocess_input
).flow_from_directory(
    directory="/content/test",
    target_size=(224,224),
    batch_size=2,
    shuffle=False
)

model.evaluate(testbatch,verbose=1,steps=30)

# show a nicely formatted classification report
labels= testbatch.classes
testp = model.predict(testbatch,steps=30)
print(classification_report(labels, testp.argmax(axis=1)))

model.save("facemaskdetectorr.model", save_format="h5")

def plot_confusion_matri(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blue):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

from sklearn.metrics import confusion_matrix
labels= testbatch.classes
testp = model.predict(testbatch,steps=30)
print(testp.argmax(axis=1),end=" ")
cm = confusion_matrix(labels,testp.argmax(axis=1))
cmlabels = ['0','1']
plot_confusion_matri(cm,cmlabels)

